---
layout: home
title: Dr. Xingjun Ma
subtitle: Associate Professor, Fudan University
---

<p style="text-align:center;">
	<img src="/assets/img/xxx.png" alt="mind and universe in Atlantean language" width="720" height="60">
</p>

<p style="text-align:justify">
I am a faculty member at the <a href="https://teai.fudan.edu.cn" target="_blank">Institute of Trustworthy Embodied AI (TEAI)</a>, Fudan University and a full-time supervisor at the <a href="https://www.sii.edu.cn/main.htm" target="_blank">Shanghai Innovation Institute</a> (上海创智学院全时导师). I also serve as an Honorary Fellow at the University of Melbourne, Australia. My primary research area is <b>Trustworthy AI</b>, with a focus on developing <b>secure</b>, <b>robust</b>, <b>explainable</b>, <b>privacy-preserving</b>, and <b>fair</b> learning algorithms and models for broad AI applications. Beyond research, I am deeply passionate about using AI to enhance our understanding of both the mind and the universe.
</p>

<p style="text-align:justify">
I received my Ph.D. from the University of Melbourne, where I also spent two wonderful years as a postdoctoral research fellow. Before joining Fudan, I worked as a lecturer at Deakin University for 1.5 years. I hold a bachelor's degree from Jilin University and a master's degree from Tsinghua University.
</p>


<!-- <p style="text-align:justify">
I received my Ph.D. degree from The University of Melbourne supervised by Prof. <a href="https://people.eng.unimelb.edu.au/baileyj/" target="_blank">James Bailey</a> and Dr. <a href="https://scholar.google.com/citations?user=MjgOHPYAAAAJ&hl=en" target="_blank">Sudanthi Wijewickrema</a>. Prior to Fudan, I spent ~2 years at The University of Melbourne as a research fellow and 1.5 years at Deakin University as a lecturer. I obtained my bachelor's and master's degrees from Jilin University and Tsinghua University, respectively.
</p> -->

<div align="center">
	<figure style="width: 80%">
	        <p>"Everything should be as simple as possible, but not simpler."</p>
	    <!-- <figcaption align="right">—<i>Albert Einstein</i></figcaption> -->
	</figure>
</div>

<p style="text-align:center">
	<a href="mailto:xingjunma@fudan.edu.cn" target="_blank">Email</a> &nbsp;/&nbsp;
    <a href="https://scholar.google.com.au/citations?user=XQViiyYAAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp;/&nbsp;
    <a href="https://github.com/xingjunm/" target="_blank">GitHub</a>
</p>

<hr style="border: 1px solid #000; margin: 20px 0;">

<p style="text-align:center">
	<font color="#0000FF">
		We are seeking motivated Master's and Ph.D. students to join our team in the areas of Trustworthy AI, MLLMs/VLMs, GenAI, and Embodied AI.</br>
		鼓励同学们通过参加<a href="https://www.sii.edu.cn/zsgz/list.htm" target="_blank">上海创智学院夏令营</a>加入我们团队（拿到优营的同学均可加入团队）。

	<!-- </br> -->
		<!-- .  -->
	</font>
</p>

<hr style="border: 1px solid #000; margin: 20px 0;">

<strong><a href="https://opentai.org/" target="_blank"> Introducing OpenTAI: Advancing Trustworthy AI Through Open Collaboration </a></strong></br></br>

Over the past two years, I’ve dedicated significant effort to building <a href="https://opentai.org/" target="_blank">OpenTAI</a>, an open platform designed to accelerate collaborative research in Trustworthy AI (TAI). Today, I’m thrilled to officially launch this initiative.</br></br>

<h6>Our Mission</h6>
OpenTAI aims to develop large-scale, practical, and open-source benchmarks, tools, and datasets—bridging the gap between TAI research and real-world applications. We’ve seeded the platform with foundational projects from our own work, but this is just the beginning.</br></br>

<h6>Call for Collaboration</h6>
OpenTAI is community-driven. We invite researchers and practitioners to:
<ul> 
<li>
Submit high-impact projects for curation (free and open to all)
	</li>
	<li>
Collaborate on expanding our resource library
</li>
</ul> 

<h6>What’s Next?</h6>
Stay tuned for a pipeline of cutting-edge benchmarks and tools in the coming year—all designed to make AI more transparent, robust, and accountable.

<p style="text-align:left;">
	<font color="#0000FF">
		Join us in shaping the future of Trustworthy AI!
	</font>
</p>

<hr style="border: 1px solid #000; margin: 20px 0;">


<h4>Books</h4>
<ul> 
	<li><strong><a href="https://ai-safety-book.github.io//" target="_blank">  Endogenous Safety in Artificial Intelligence (《人工智能内生安全》)</a></strong>
	</li>

	<li><strong><a href="https://ai-data-model-safety.github.io/" target="_blank"> Artificial Intelligence: Data and Model Safety (《人工智能：数据与模型安全》) </a></strong>
	</li>
</ul>

<hr>


<h3>News</h3>

<ul> 

	<li><strong>[09/2025]</strong> Our survey paper <a href="https://arxiv.org/abs/2502.05206" target="_blank">Safety at Scale: A Comprehensive Survey of Large Model and Agent Safety</a> has been published in <i>Foundations and Trends® in Privacy and Security</i>.</li>

	<li><strong>[08/2025]</strong> Our work on <a href="https://arxiv.org/abs/2401.15295" target="_blank">multi-trigger backdoor attacks</a> has been accepted to TDSC.</li>

	<li><strong>[08/2025]</strong> Our paper <a href="https://arxiv.org/abs/2205.12709" target="_blank">VeriFi: Towards Verifiable Federated Unlearning</a> has been selected as the <strong style="color:red;">Runner-up for the 2024 Best Paper Award</strong> by the IEEE TDSC journal. Congratulations to all co-authors!</li>

	<li><strong>[07/2025]</strong> Four papers have been accepted to ACM Multimedia 2025. </li>

	<li><strong>[06/2025]</strong> Three papers have been accepted to ICCV 2025. </li>

	<li><strong>[05/2025]</strong> Our <a href="https://github.com/bboylyg/BackdoorLLM" target="_blank">BackdoorLLM Benchmark</a> received <strong style="color:red;">First Prize</strong> in the <a href="https://www.mlsafety.org/safebench/winners" target="_blank">SafeBench Competition</a>, organized by the <a href="https://www.safe.ai/" target="_blank">Center for AI Safety</a>. Congratulations to all co-authors! </li>

	<li><strong>[05/2025]</strong> Our work on super transferable attacks <a href="https://arxiv.org/abs/2505.05528" target="_blank"> X-Transfer Attacks </a> has been accepted to ICML 2025. </li>

	<li><strong>[03/2025]</strong> The preprint of our long survey paper <a href="https://arxiv.org/abs/2502.05206" target="_blank"> Safety at Scale: A Comprehensive Survey of Large Model and Agent Safety </a> is available on arXiv. Many thanks to all collaborators! </li>

	<li><strong>[02/2025]</strong> Our works on <a href="https://arxiv.org/pdf/2411.15210" target="_blank">Million-scale Adversarial Robustness Evalution</a>, <a href="https://arxiv.org/abs/2411.13136" target="_blank">Test-time Adversarial Prompt Tuning</a>, and <a href="https://arxiv.org/abs/2410.05346" target="_blank">AnyAttack</a> have been accepted to CVPR 2025. </li>

	<li><strong>[01/2025]</strong> Our works on <a href="https://arxiv.org/abs/2410.20971" target="_blank">RL-based jailbreak defense for VLMs</a> and <a href="https://arxiv.org/abs/2502.01385" target="_blank">backdoor sample detection in CLIP</a> have been accepted to ICLR 2025. </li>

	<li><strong>[12/2024]</strong> I will serve as an Area Chair for ICML 2025. </li>

	<li><strong>[12/2024]</strong> Our works on <a href="https://arxiv.org/abs/2501.01106" target="_blank">targeted transferable adversarial attack</a>, <a href="https://arxiv.org/pdf/2501.01090" target="_blank">defense against model extraction attacks</a>, and <a href="https://arxiv.org/abs/2501.02997" target="_blank">RL-based LLM auditing</a> have been accepted to AAAI 2025. </li>

	<li><strong>[09/2024]</strong> I will serve as an Area Chair for ICLR 2025. </li>

	<li><strong>[09/2024]</strong> One paper on <a href="https://arxiv.org/abs/2410.09909v1" target="_blank"> unlearnable examples for segmentation models </a> has been accepted to NeurIPS 2024. </li>

	<li><strong>[07/2024]</strong> Our works on <a href="https://arxiv.org/abs/2405.16285" target="_blank"> model lock </a>, <a href="https://www.arxiv.org/abs/2408.01978" target="_blank"> detecting query-based adversarial attacks </a>, and <a href="https://arxiv.org/abs/2405.17894" target="_blank"> multimodal jailbreak attacks on VLMs </a> have been accepted to MM 2024. </li>

	<li><strong>[07/2024]</strong> Our work on <a href="https://arxiv.org/abs/2311.11261" target="_blank"> adversarial prompt tuning </a> has been accepted to accepted by ECCV 2024. </li>

	<li><strong>[04/2024]</strong> Our work on <a href="" target="_blank"> intrinsic motivation for RL </a> has been accepted to IJCAI 2024. </li>

	<li><strong>[03/2024]</strong> Our work on <a href="https://arxiv.org/abs/2305.02605" target="_blank"> adversarial policy learning in RL </a> is accepted by DSN 2024. </li>

	<li><strong>[03/2024]</strong> Our work on <a href="https://arxiv.org/abs/2311.05915" target="_blank"> safety alignment of LLMs </a> is accepted by NAACL 2024. </li>

	<li><strong>[03/2024]</strong> Our work on <a href="https://arxiv.org/abs/2205.12709" target="_blank"> federated machine unlearning </a> has been accepted to  TDSC. </li>

	<li><strong>[01/2024]</strong> Our work on <a href="https://openreview.net/pdf?id=oZyAqjAjJW" target="_blank">self-supervised learning</a> have been accepted to ICLR 2024. </li>

	<!-- <li><strong>[4/2023]</strong> Our work on <a href="https://arxiv.org/abs/2305.14876" target="_blank"> backdoor defense </a> is accepted by ICML'23. </li>

	<li><strong>[3/2023]</strong> Our work on <a href="https://arxiv.org/abs/2006.13726" target="_blank"> margin decomposition adversarial attack </a> is accepted by Machine Learning journal. </li>

	<li><strong>[2/2023]</strong> Our work on <a href="https://arxiv.org/abs/2301.01217" target="_blank"> unlearnable examples </a> is accepted by CVPR'23. </li>

	<li><strong>[1/2023]</strong> Our work <a href="https://link.springer.com/article/10.1007/s10853-022-07793-6" target="_blank">Machine learning guided alloy design of high-temperature NiTiHf shape memory alloys</a> recieves the <a href="https://link.springer.com/article/10.1007/s10853-023-08250-8#:~:text=The%20winner%20of%20the%202022,Corujeira%20Gallo%2C%20and%20Wei%20Xu." target="_blank" style = "color: red">2022 Robert W. Cahn Best Paper Award</a> at Journal of Materials Science. Congrats to all the authors!</li>

	<li><strong>[1/2023]</strong> Our works on <a href="https://arxiv.org/abs/2301.10908" target="_blank"> Cognitive Backdoor Distillation </a> and <a href="https://openreview.net/forum?id=-htnolWDLvP" target="_blank"> Transferable Unlearnable Examples </a>  are accepted by ICLR'23. </li>

	<li><strong>[1/1/2023]</strong> Merry christmas and happy new year! -->

	<!-- <li><strong>[11/2022]</strong> Our work on <a href="https://arxiv.org/abs/2211.07915" target="_blank"> Time Series Backdoor Attacks </a> is accepted by <a href="https://satml.org/" target="_blank"> SaTML 2023 </a></li>

	<li><strong>[10/2022]</strong> Our survey paper on <a href="https://arxiv.org/abs/2012.06337" target="_blank"> Privacy and Robustness of Federeatd Learning </a> is accepted by TNNLS</li>

	<li><strong>[10/2022]</strong> Our work on <a href="https://arxiv.org/abs/2210.09545" target="_blank"> Backdoor Defense of Fine-tuned Language Models </a> is accepted by EMNLP'22</li>

	<li><strong>[09/2022]</strong> Our theoretical work on <a href="https://www.mdpi.com/1099-4300/24/9/1220" target="_blank"> Local Intrinsic Dimensionality, Entropy and Statistical Divergences </a> is accepted by Entropy. </li>

	<li><strong>[09/2022]</strong> Our work on <a href="https://arxiv.org/abs/2205.14926" target="_blank"> Federated Adversarial Training </a> is accepted by NeurIPS'22. </li>

	<li><strong>[08/2022]</strong> Our work <a href="https://arxiv.org/abs/2207.05641" target="_blank"> Backdoor Attack on Crowd Counting </a> is accepted by ACMMM’22. </li>

	<li><strong>[01/2022]</strong> Our work on <a href="https://openreview.net/pdf?id=qSV5CuSaK_a" target="_blank"> Visual Object Tracking (VOT) Backdoor</a> is accepted by ICLR'22. </li>

	<li><strong>[12/2021]</strong> Our work on <a href="https://arxiv.org/abs/2112.05588" target="_blank">Copyright Protection of Deep Learning Models</a> is accepted by IEEE S&P'22. </li>

	<li><strong>[10/2021]</strong> Our work <a href="https://www.sisap.org/2021/awards.html" target="_blank">Relationships between Local Intrinsic Dimensionality and Tail Entropy</a> recieves the <strong><font color="red">Best Paper Award</font></strong> at SISAP'21.</li>

	<li><strong>[09/2021]</strong> Our works on <a href="https://arxiv.org/abs/2110.11571" target="_blank">Anti-Backdoor Learning</a>, <a href="https://arxiv.org/abs/2110.03825" target="_blank">Robust Neural Architecture</a>, <a href="http://arxiv.org/abs/2110.13675" target="_blank">Alpha-IoU Loss</a> and <a href="" target="_blank">Fairness in Collabarative Learning</a> are accepted by NeurIPS'21. </li>

	<li><strong>[08/2021]</strong> I will serve as a SPC Member for AAAI'22. </li>
	
	<li><strong>[07/2021]</strong> Our work on <a href="https://arxiv.org/abs/2108.07969" target="_blank">Adversarial Robustness Distillation</a> is accepted by ICCV'21. </li>

	<li><strong>[07/2021]</strong> Our work  <a href="https://ssdbm.org/2021/awards" target="_blank">Sub-trajectory Similarity Join with Obfuscation</a> recieves the <strong><font color="red">Best Paper Runner-up Award</font></strong> at SSDBM'21, congrats to all the authors.</li>
	
	<li><strong>[06/2021]</strong> Our work  <a href="https://hanxunh.github.io/Unlearnable-Examples/" target="_blank">Unlearnable Examples: Making Personal Data Unexploitable</a> is featured by <a href="https://www.technologyreview.com/2021/05/05/1024613/stop-ai-recognizing-your-face-selfies-machine-learning-facial-recognition-clearview" target="_blank">MIT Technology Review</a> and <a href="https://pursuit.unimelb.edu.au/articles/blocking-ai-to-keep-your-personal-data-your-own" target="_blank">PURSUIT</a>.</li>

	<li><strong>[05/2021]</strong> I am recognized as an <a href="https://iclr.cc/Conferences/2021/Reviewers"  target="_blank">Outstanding Reviewer</a> for ICLR'21.</li>

	<li><strong>[04/2021]</strong> Our work on  <a href="https://arxiv.org/abs/2106.01532" target="_blank">Inpainting Detection</a> is accepted by IJCAI'21.</li>


	<li><strong>[02/2021]</strong> I received the Mini ARC Analog Programme (MAAP) - Discovery Grant from Deakin University as a Co-PI.</li>

	<li><strong>[01/2021]</strong> Our works on <a href="https://openreview.net/forum?id=iAmZUo0DxC0" target="_blank">Data Protection</a>, <a href="https://openreview.net/forum?id=zQTezqCCtNx" target="_blank">Adversarial Defense</a>  and <a href="https://openreview.net/forum?id=9l0K4OM-oXE" target="_blank">Backdoor Defense</a> are accepted by ICLR'21.</li> -->
 
</ul>

<hr>
<h3> Research Interests</h3>
<ul>
	<li> Trustworthy AI
		<ul>
			<li> Adversarial/jailbreak attacks and defenses </li>
			<li> Backdoor attacks and defenses</li>
			<li> Reinforcement learning, safety alignment </li>
			<li> Data privacy, data/model extraction </li>
			<li> Memorization, data attribution, unlearning  </li>
		</ul>
	</li>
	<li> Multimodal and Generative AI
		<ul>
			<li> Multimodal learning, vision-language models </li>
			<li> Diffuson models, Text2Image generation, Text2Video generation </li>
			<li> World model, embodied AI </li>
		</ul>
	</li>
</ul>

<hr>

<!-- <h3> PhD Students I currently (co-)supervise</h3>

<ul>
	<li> Yifeng Gao (Fudan, 2023 - ) </li>
	<li> Yifan Ding (Fudan, 2023 - ) </li>
	<li> Nuofan Wang (Fudan, 2023 - ) </li>
	<li> Ye Sun (Fudan, 2023 - ) </li>
	<li> Yixu Wang (Fudan, 2023 - ) </li>
	<li> Kun Zhai (Fudan, 2022 - ) </li>
	<li> Xin Wang (Fudan, 2022 - ) </li>
	<li> Teng Li (Fudan, 2022 - ) </li>
	<li> Xueqi Ma (UoM, 2022 - ) </li> -->
	<!-- <li> Hanxun Huang (UoM, 2021 - ) </li> -->
	<!-- <li> Yujing Jiang (UoM, 2020 - ) </li> -->
	<!-- <li> Siqi Xia (Deakin) </li>
	<li> Chuxuan Tong (Deakin) </li>
	<li> Xinzhe Li (Deakin) </li>
	<li> Saheed Adebayo Tijani (Deakin) </li>
	<li> Zichan Ruan (Deakin) </li>
	<li> Gayathri Radhabai Gopinathan Nair (Deakin) </li> -->
<!-- </ul> -->

<!-- <hr> -->

<h3> Professional Activities</h3>

<ul>
	<li> <b>Program Committee Member</b>
		<ul>
			<li>ICLR (2019-2025), ICML (2019-2025), NeurIPS (2019-2024), CVPR (2020-2025), ICCV (2021-2023), ECCV (2020), AAAI (2020-2022), IJCAI (2020-2021), KDD (2019,2021), ICDM (2021), SDM (2021), AICAI (2021)</li>
			<!-- <li>ICML: 2019, 2020, 2021, 2022</li>
			<li>NeurIPS: 2019, 2020, 2021, 2022</li>
			<li>CVPR: 2020, 2021, 2022</li>
			<li>ICCV: 2021</li>
			<li>ECCV: 2020</li>
			<li>AAAI: 2020, 2021, 2022</li>
			<li>IJCAI: 2020, 2021</li>
			<li>KDD: 2019, 2021</li>
			<li>ICDM: 2021</li>
			<li>SDM: 2021</li>
			<li>AJCAI: 2021</li> -->
		</ul>
	</li>

	<li> <b>Journal Reviewer</b>
		<ul>
			<li>Nature Communications, Pattern Recognition, TPAMI, TIP, IJCV, JAIR, TNNLS, TKDE, TIFS, TOMM, KAIS</li>
			<!-- <li>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
			<li>IEEE Transactions on Image Processing (TIP)</li>
			<li>International Journal of Computer Vision (IJCV)</li>
			<li>Pattern Recognition</li>
			<li>Journal of Artificial Intelligence Research (JAIR)</li>
			<li>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</li>
			<li>IEEE Transactions on Knowledge and Data Engineering (TKDE)</li>
			<li>Transactions on Information Forensics and Security (TIFS)</li>
			<li>ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</li>
			<li>Knowledge and Information Systems (KAIS)</li>
			<li>Neurocomputing</li>
			<li>IEEE Robotics and Automation Letters (RA-L)</li>
			<li>Pattern Recognition Letters</li> -->
		</ul>
	</li>

</ul>

